{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a re-creation of [Text classification from scratch](https://keras.io/examples/nlp/text_classification_from_scratch/) for internalization purposes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "This example shows how to do text classification starting from raw text. TextVectorization is used for word splitting & indexing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "from acquire import prep_and_split_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data: \n",
    "\n",
    "Let's acquire the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows in training set: 37931\n",
      "Number of rows in validation set: 2108\n",
      "Number of rows in test set: 2107\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda3/lib/python3.8/site-packages/pandas/core/frame.py:4308: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  return super().drop(\n"
     ]
    }
   ],
   "source": [
    "train, validate, test = prep_and_split_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = train.basic_clean_v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "case_0 = train[train.case == 0].basic_clean_v2\n",
    "case_1 = train[train.case == 1].basic_clean_v2\n",
    "case_2 = train[train.case == 2].basic_clean_v2\n",
    "case_3 = train[train.case == 3].basic_clean_v2\n",
    "case_4 = train[train.case == 4].basic_clean_v2\n",
    "case_5 = train[train.case == 5].basic_clean_v2\n",
    "case_6 = train[train.case == 6].basic_clean_v2\n",
    "case_7 = train[train.case == 7].basic_clean_v2\n",
    "case_8 = train[train.case == 8].basic_clean_v2\n",
    "case_9 = train[train.case == 9].basic_clean_v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_case_0 = validate[validate.case == 0].basic_clean_v2\n",
    "val_case_1 = validate[validate.case == 1].basic_clean_v2\n",
    "val_case_2 = validate[validate.case == 2].basic_clean_v2\n",
    "val_case_3 = validate[validate.case == 3].basic_clean_v2\n",
    "val_case_4 = validate[validate.case == 4].basic_clean_v2\n",
    "val_case_5 = validate[validate.case == 5].basic_clean_v2\n",
    "val_case_6 = validate[validate.case == 6].basic_clean_v2\n",
    "val_case_7 = validate[validate.case == 7].basic_clean_v2\n",
    "val_case_8 = validate[validate.case == 8].basic_clean_v2\n",
    "val_case_9 = validate[validate.case == 9].basic_clean_v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_case_0 = test[test.case == 0].basic_clean_v2\n",
    "test_case_1 = test[test.case == 1].basic_clean_v2\n",
    "test_case_2 = test[test.case == 2].basic_clean_v2\n",
    "test_case_3 = test[test.case == 3].basic_clean_v2\n",
    "test_case_4 = test[test.case == 4].basic_clean_v2\n",
    "test_case_5 = test[test.case == 5].basic_clean_v2\n",
    "test_case_6 = test[test.case == 6].basic_clean_v2\n",
    "test_case_7 = test[test.case == 7].basic_clean_v2\n",
    "test_case_8 = test[test.case == 8].basic_clean_v2\n",
    "test_case_9 = test[test.case == 9].basic_clean_v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validate.case.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' This is how I saved each row as a text file. If I ever have to do this again I will think of a way to make it iterable'''\n",
    "\n",
    "'''\n",
    "file = '/Users/brent/Leashed/test/case-0/{}.txt'\n",
    "for i, row in test[test.case == 0].iterrows():\n",
    "    with open(file.format('note_'+str(i)), 'w') as f:\n",
    "        f.write(str(row['basic_clean_v2']))\n",
    "file = '/Users/brent/Leashed/test/case-1/{}.txt'\n",
    "for i, row in test[test.case == 1].iterrows():\n",
    "    with open(file.format('note_'+str(i)), 'w') as f:\n",
    "        f.write(str(row['basic_clean_v2']))\n",
    "file = '/Users/brent/Leashed/test/case-2/{}.txt'\n",
    "for i, row in test[test.case == 2].iterrows():\n",
    "    with open(file.format('note_'+str(i)), 'w') as f:\n",
    "        f.write(str(row['basic_clean_v2']))\n",
    "file = '/Users/brent/Leashed/test/case-3/{}.txt'\n",
    "for i, row in test[test.case == 3].iterrows():\n",
    "    with open(file.format('note_'+str(i)), 'w') as f:\n",
    "        f.write(str(row['basic_clean_v2']))\n",
    "file = '/Users/brent/Leashed/test/case-4/{}.txt'\n",
    "for i, row in test[test.case == 4].iterrows():\n",
    "    with open(file.format('note_'+str(i)), 'w') as f:\n",
    "        f.write(str(row['basic_clean_v2']))\n",
    "file = '/Users/brent/Leashed/test/case-5/{}.txt'\n",
    "for i, row in test[test.case == 5].iterrows():\n",
    "    with open(file.format('note_'+str(i)), 'w') as f:\n",
    "        f.write(str(row['basic_clean_v2']))\n",
    "file = '/Users/brent/Leashed/test/case-6/{}.txt'\n",
    "for i, row in test[test.case == 6].iterrows():\n",
    "    with open(file.format('note_'+str(i)), 'w') as f:\n",
    "        f.write(str(row['basic_clean_v2']))\n",
    "file = '/Users/brent/Leashed/test/case-7/{}.txt'\n",
    "for i, row in test[test.case == 7].iterrows():\n",
    "    with open(file.format('note_'+str(i)), 'w') as f:\n",
    "        f.write(str(row['basic_clean_v2']))\n",
    "file = '/Users/brent/Leashed/test/case-8/{}.txt'\n",
    "for i, row in test[test.case == 8].iterrows():\n",
    "    with open(file.format('note_'+str(i)), 'w') as f:\n",
    "        f.write(str(row['basic_clean_v2']))\n",
    "file = '/Users/brent/Leashed/test/case-9/{}.txt'\n",
    "for i, row in test[test.case == 9].iterrows():\n",
    "    with open(file.format('note_'+str(i)), 'w') as f:\n",
    "        f.write(str(row['basic_clean_v2']))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 37931 files belonging to 10 classes.\n",
      "Found 2118 files belonging to 10 classes.\n",
      "Found 2107 files belonging to 10 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-28 21:08:35.408236: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "train_dataset = tf.keras.preprocessing.text_dataset_from_directory('/Users/brent/Leashed/student_notes/', batch_size=32)\n",
    "validate_dataset = tf.keras.preprocessing.text_dataset_from_directory('/Users/brent/Leashed/validate/', batch_size=32)\n",
    "test_dataset = tf.keras.preprocessing.text_dataset_from_directory('/Users/brent/Leashed/test/', batch_size=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of batches in train_dataset: 1186\n",
      "Number of batches in validate_dataset: 67\n",
      "Number of batches in test_dataset: 66\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of batches in train_dataset: {train_dataset.cardinality()}')\n",
    "print(f\"Number of batches in validate_dataset: {validate_dataset.cardinality()}\")\n",
    "print(f\"Number of batches in test_dataset: {test_dataset.cardinality()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's preview a few samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'mrs .  wicks is a 67 year old female who presents with 3 week history of sleeping touble .  she reports trouble initiating sleep with frequent awakenings and early orning awakenings .  sleep is now from 12am-4 5am .  before this period she would sleep a full night .  of note pts son passed away recently .  pt reports anhedonia ,  depressed mood ,  increased appetite ;  denies decreased concentration ,  feelings of guilt ,  si hi ,  skin changes ,  hair loss ,  constipation ,  n v d ,  or snoring sleep apbea .  the patient endorses hallucinations :  she saw her deceased son at the kitchen table recently .  she also reports hearing music at night when there apparently was none .  pt has been taking ambien x5 nights to help with sleep to no avail .      pmh :  htn ,  breast cancer x10 years ago in remission  fh :  depression in mom  rx :  hctz'\n",
      "8\n",
      "b'cc :  i am having stomach pain  hpi :  ch 35 yo m presents with dull ,  gnawing ,  worsening upper epigastric pain .  pain began 2 months ago ,  usually takes tums to alleviate pain but has not been working .  patient rates pain as 5 10 .  pain has also increased in frequency to 2x day from few episodes per week .  pain is not related to food ,  does not radiate anywhere and sometimes awakens patient from sleep .  patient also has bloating after eating and 2wk history of dark brown stool that is normal consistency .    ros :  + nausea ,  - for vomiting ,  weight loss ,  fatty stool ,  or blood in stool  pmh :  lower back pain  medications :  motrin  allergies :  none  fh :  uncle with bleeding ulcer  psh :  none  sex h :  not sexually active ,  - sti test recent  social h :  15 pk yr smoker ,  quit alcohol ,  no illicit drugs ,  exercises daily  '\n",
      "3\n",
      "b\"67 yo f c o difficulty falling asleep since past 3 weeks after her son's death   difficulty falling asleep , multiple awakening , difficulty staying asleep , difficulty to sleep back after awakening .   also reports od decreased energy level , increased appetite ,   difficulty concentration , decreased cincentration ,  , decrease in concentration  -denies restlessness , slowed down , head ache , feeling of guilt .   h o good suppoting system  ros ; neg except as above  allergy ; nkda  meds ; hctz , lisinopril , ambien  pmh ; htn , h o breast cancer  psh ; appendectomy at age 20 and lumpectomy  fh ; father died of sroke , mother is healthy and alive and has h o depression  sh ; no tobacco drugs . exercise regularly , retired  , lives with husband and sexually active with husband , occ etoh\"\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "# It's important to take a look at your raw data to ensure your normalization\n",
    "# and tokenization will work as expected. We can do that by taking a few\n",
    "# examples from the training set and looking at them.\n",
    "# This is one of the places where eager execution shines:\n",
    "# we can just evaluate these tensors using .numpy()\n",
    "# instead of needing to evaluate them in a Session/Graph context.\n",
    "for text_batch, label_batch in train_dataset.take(1):\n",
    "    for i in range(3):\n",
    "        print(text_batch.numpy()[i])\n",
    "        print(label_batch.numpy()[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       ",            429737\n",
       ".            372297\n",
       ":            178467\n",
       "and          126653\n",
       "no           114796\n",
       "              ...  \n",
       "paralysis        21\n",
       "limiting         21\n",
       "ws               21\n",
       "-nkda            21\n",
       "markers          21\n",
       "Length: 4722, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = pd.Series(' '.join(train.basic_clean_v2).split()).value_counts()\n",
    "\n",
    "b[b > 20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import TextVectorization\n",
    "import string\n",
    "import re\n",
    "\n",
    "def custom_standardization(input_data):\n",
    "    lowercase = tf.strings.lower(input_data)\n",
    "    stripped_html = tf.strings.regex_replace(lowercase, \"<br />\", \" \")\n",
    "    return tf.strings.regex_replace(\n",
    "        stripped_html, f\"[{re.escape(string.punctuation)}]\", \"\"\n",
    "    )\n",
    "\n",
    "max_features = 5000\n",
    "embedding_dim = 28\n",
    "sequence_length = 170"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'TextVectorization' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/jl/s3ptdwdx55v01d2g2wrs7vdc0000gn/T/ipykernel_18464/2418802493.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m vectorize_layer = TextVectorization(\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mstandardize\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mcustom_standardization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mmax_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax_features\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0moutput_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'int'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0moutput_sequence_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msequence_length\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'TextVectorization' is not defined"
     ]
    }
   ],
   "source": [
    "vectorize_layer = TextVectorization(\n",
    "    standardize= custom_standardization,\n",
    "    max_tokens = max_features,\n",
    "    output_mode='int',\n",
    "    output_sequence_length = sequence_length\n",
    ")\n",
    "# Set vocabulary for the text encoder\n",
    "vectorize_layer.adapt(train_dataset.map(lambda text, label: text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's make a text-only dataset (no labels):\n",
    "text_ds = train_dataset.map(lambda x, y: x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorize_layer.adapt(text_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorize_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_text(text, label):\n",
    "    text = tf.expand_dims(text, -1)\n",
    "    return vectorize_layer(text), label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize the data.\n",
    "train_ds = train_dataset.map(vectorize_text)\n",
    "val_ds = validate_dataset.map(vectorize_text)\n",
    "test_ds = test_dataset.map(vectorize_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do async prefetching / buffering of the data for best performance on GPU.\n",
    "train_ds = train_ds.cache().prefetch(buffer_size=10)\n",
    "val_ds = val_ds.cache().prefetch(buffer_size=10)\n",
    "test_ds = test_ds.cache().prefetch(buffer_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 575   42   69   86   14    7  336    4  230  333  510  205  111   59\n",
      "  499   13    7   40   39   43    2  154    1   11  711 2602  205  443\n",
      "  205   32  198   55    4  638   14  217  128  170   54   20   30   19\n",
      "   38   54  638    4  171  773   95    9    2  638  333  142  305   12\n",
      "    3    9    4  459   16    4  620   71  305   12    3  128    4  459\n",
      "   16  620   71  396    6  252  584  149    5  144    8 4115   13    3\n",
      "  182    9  379   65    6  603  252  584   94   11  796    1    2    1\n",
      "  203  444  209   29 1194  331   16  252    1   34   45    2   77   84\n",
      "  215 1171   32   36   23  152    3  281   44  781  152    1    2 2879\n",
      "   35    1    2  252 4115   70   76   68   12  161  436  868   74   21\n",
      "  388   78  122  113  517    6  365   49 1443 1358    3   89   33   62\n",
      "   48    4  131    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0]\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "for text_batch, label in train_ds.take(1):\n",
    "    for i in range(1):\n",
    "        print(text_batch.numpy()[i])\n",
    "        print(label.numpy()[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "\n",
    "# A integer input for vocab indices.\n",
    "inputs = tf.keras.Input(shape=(None,), dtype=\"int64\")\n",
    "\n",
    "# Next, we add a layer to map those vocab indices into a space of dimensionality\n",
    "# 'embedding_dim'.\n",
    "x = layers.Embedding(max_features, embedding_dim)(inputs)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "\n",
    "# Conv1D + global max pooling\n",
    "x = layers.Conv1D(128, 7, padding=\"valid\", activation=\"relu\", strides=3)(x)\n",
    "x = layers.Conv1D(128, 7, padding=\"valid\", activation=\"relu\", strides=3)(x)\n",
    "x = layers.GlobalMaxPooling1D()(x)\n",
    "\n",
    "# We add a vanilla hidden layer:\n",
    "x = layers.Dense(128, activation=\"relu\")(x)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "\n",
    "# We project onto a single unit output layer, and squash it with a sigmoid:\n",
    "predictions = layers.Dense(1, activation=\"sigmoid\", name=\"predictions\")(x)\n",
    "\n",
    "model = tf.keras.Model(inputs, predictions)\n",
    "\n",
    "# Compile the model with binary crossentropy loss and an adam optimizer.\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 3\n",
    "\n",
    "model.fit(train_ds, validation_data=val_ds, epochs = epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the model on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A string input\n",
    "inputs = tf.keras.Input(shape=(1,), dtype=\"string\")\n",
    "# Turn strings into vocab indices\n",
    "indices = vectorize_layer(inputs)\n",
    "# Turn vocab indices into predictions\n",
    "outputs = model(indices)\n",
    "\n",
    "# Our end to end model\n",
    "end_to_end_model = tf.keras.Model(inputs, outputs)\n",
    "end_to_end_model.compile(\n",
    "    loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test it with `raw_test_ds`, which yields raw strings\n",
    "end_to_end_model.evaluate(text_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3067ead486e059ec00ffe7555bdb889e6e264a24dc711bf108106cc7baee8d5d"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
