{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a re-creation of [Text classification from scratch](https://keras.io/examples/nlp/text_classification_from_scratch/) for internalization purposes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "This example shows how to do text classification starting from raw text. TextVectorization is used for word splitting & indexing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "from acquire import prep_and_split_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data: \n",
    "\n",
    "Let's acquire the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows in training set: 37931\n",
      "Number of rows in validation set: 2108\n",
      "Number of rows in test set: 2107\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda3/lib/python3.8/site-packages/pandas/core/frame.py:4308: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  return super().drop(\n"
     ]
    }
   ],
   "source": [
    "train, validate, test = prep_and_split_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = train.basic_clean_v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36542    67 yo f present with trouble sleeping .  pt st...\n",
       "40005    20yof presenting with ha .  she said the ha st...\n",
       "8904     patient is a 35 year old male with epigastric ...\n",
       "41710    20 yo f with 2 day histoy of all over headache...\n",
       "41253    pt presents with severe diffuse headache .  ca...\n",
       "                               ...                        \n",
       "6709     35yo m w 2mo epigastric pain + nausea .    pat...\n",
       "26324    hpi : 26 yo f c o palpitation and shortness of...\n",
       "33967    l . w .  is a 67 year old female with complain...\n",
       "22962    26 yo female follwup for ed visit due to palpa...\n",
       "33928    67 yo f presenting with new-onset insomnia for...\n",
       "Name: basic_clean_v2, Length: 37931, dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "case_0 = train[train.case == 0].basic_clean_v2\n",
    "case_1 = train[train.case == 1].basic_clean_v2\n",
    "case_2 = train[train.case == 2].basic_clean_v2\n",
    "case_3 = train[train.case == 3].basic_clean_v2\n",
    "case_4 = train[train.case == 4].basic_clean_v2\n",
    "case_5 = train[train.case == 5].basic_clean_v2\n",
    "case_6 = train[train.case == 6].basic_clean_v2\n",
    "case_7 = train[train.case == 7].basic_clean_v2\n",
    "case_8 = train[train.case == 8].basic_clean_v2\n",
    "case_9 = train[train.case == 9].basic_clean_v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_case_0 = validate[validate.case == 0].basic_clean_v2\n",
    "val_case_1 = validate[validate.case == 1].basic_clean_v2\n",
    "val_case_2 = validate[validate.case == 2].basic_clean_v2\n",
    "val_case_3 = validate[validate.case == 3].basic_clean_v2\n",
    "val_case_4 = validate[validate.case == 4].basic_clean_v2\n",
    "val_case_5 = validate[validate.case == 5].basic_clean_v2\n",
    "val_case_6 = validate[validate.case == 6].basic_clean_v2\n",
    "val_case_7 = validate[validate.case == 7].basic_clean_v2\n",
    "val_case_8 = validate[validate.case == 8].basic_clean_v2\n",
    "val_case_9 = validate[validate.case == 9].basic_clean_v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_case_0 = test[test.case == 0].basic_clean_v2\n",
    "test_case_1 = test[test.case == 1].basic_clean_v2\n",
    "test_case_2 = test[test.case == 2].basic_clean_v2\n",
    "test_case_3 = test[test.case == 3].basic_clean_v2\n",
    "test_case_4 = test[test.case == 4].basic_clean_v2\n",
    "test_case_5 = test[test.case == 5].basic_clean_v2\n",
    "test_case_6 = test[test.case == 6].basic_clean_v2\n",
    "test_case_7 = test[test.case == 7].basic_clean_v2\n",
    "test_case_8 = test[test.case == 8].basic_clean_v2\n",
    "test_case_9 = test[test.case == 9].basic_clean_v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3    507\n",
       "5    347\n",
       "4    269\n",
       "9    246\n",
       "8    215\n",
       "7    199\n",
       "0    112\n",
       "2     99\n",
       "6     77\n",
       "1     37\n",
       "Name: case, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validate.case.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nfile = 'path/test/case-0{}.txt'\\nfor i, row in test[test.case == 0].iterrows():\\n    with open(file.format('note_'+str(i)), 'w') as f:\\n        f.write(str(row['basic_clean_v2']))\\nfile = 'path/test/case-1/{}.txt'\\nfor i, row in test[test.case == 1].iterrows():\\n    with open(file.format('note_'+str(i)), 'w') as f:\\n        f.write(str(row['basic_clean_v2']))\\nfile = 'path/test/case-2/{}.txt'\\nfor i, row in test[test.case == 2].iterrows():\\n    with open(file.format('note_'+str(i)), 'w') as f:\\n        f.write(str(row['basic_clean_v2']))\\nfile = 'path/test/case-3/{}.txt'\\nfor i, row in test[test.case == 3].iterrows():\\n    with open(file.format('note_'+str(i)), 'w') as f:\\n        f.write(str(row['basic_clean_v2']))\\nfile = 'path/test/case-4/{}.txt'\\nfor i, row in test[test.case == 4].iterrows():\\n    with open(file.format('note_'+str(i)), 'w') as f:\\n        f.write(str(row['basic_clean_v2']))\\nfile = 'path/test/case-5/{}.txt'\\nfor i, row in test[test.case == 5].iterrows():\\n    with open(file.format('note_'+str(i)), 'w') as f:\\n        f.write(str(row['basic_clean_v2']))\\nfile = 'path/test/case-6/{}.txt'\\nfor i, row in test[test.case == 6].iterrows():\\n    with open(file.format('note_'+str(i)), 'w') as f:\\n        f.write(str(row['basic_clean_v2']))\\nfile = 'path/test/case-7/{}.txt'\\nfor i, row in test[test.case == 7].iterrows():\\n    with open(file.format('note_'+str(i)), 'w') as f:\\n        f.write(str(row['basic_clean_v2']))\\nfile = 'path/test/case-8/{}.txt'\\nfor i, row in test[test.case == 8].iterrows():\\n    with open(file.format('note_'+str(i)), 'w') as f:\\n        f.write(str(row['basic_clean_v2']))\\nfile = 'path/test/case-9/{}.txt'\\nfor i, row in test[test.case == 9].iterrows():\\n    with open(file.format('note_'+str(i)), 'w') as f:\\n        f.write(str(row['basic_clean_v2']))\\n\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''' This is how I saved each row as a text file. If I ever have to do this again I will think of a way to make it iterable'''\n",
    "\n",
    "'''\n",
    "file = 'path/test/case-0{}.txt'\n",
    "for i, row in test[test.case == 0].iterrows():\n",
    "    with open(file.format('note_'+str(i)), 'w') as f:\n",
    "        f.write(str(row['basic_clean_v2']))\n",
    "file = 'path/test/case-1/{}.txt'\n",
    "for i, row in test[test.case == 1].iterrows():\n",
    "    with open(file.format('note_'+str(i)), 'w') as f:\n",
    "        f.write(str(row['basic_clean_v2']))\n",
    "file = 'path/test/case-2/{}.txt'\n",
    "for i, row in test[test.case == 2].iterrows():\n",
    "    with open(file.format('note_'+str(i)), 'w') as f:\n",
    "        f.write(str(row['basic_clean_v2']))\n",
    "file = 'path/test/case-3/{}.txt'\n",
    "for i, row in test[test.case == 3].iterrows():\n",
    "    with open(file.format('note_'+str(i)), 'w') as f:\n",
    "        f.write(str(row['basic_clean_v2']))\n",
    "file = 'path/test/case-4/{}.txt'\n",
    "for i, row in test[test.case == 4].iterrows():\n",
    "    with open(file.format('note_'+str(i)), 'w') as f:\n",
    "        f.write(str(row['basic_clean_v2']))\n",
    "file = 'path/test/case-5/{}.txt'\n",
    "for i, row in test[test.case == 5].iterrows():\n",
    "    with open(file.format('note_'+str(i)), 'w') as f:\n",
    "        f.write(str(row['basic_clean_v2']))\n",
    "file = 'path/test/case-6/{}.txt'\n",
    "for i, row in test[test.case == 6].iterrows():\n",
    "    with open(file.format('note_'+str(i)), 'w') as f:\n",
    "        f.write(str(row['basic_clean_v2']))\n",
    "file = 'path/test/case-7/{}.txt'\n",
    "for i, row in test[test.case == 7].iterrows():\n",
    "    with open(file.format('note_'+str(i)), 'w') as f:\n",
    "        f.write(str(row['basic_clean_v2']))\n",
    "file = 'path/test/case-8/{}.txt'\n",
    "for i, row in test[test.case == 8].iterrows():\n",
    "    with open(file.format('note_'+str(i)), 'w') as f:\n",
    "        f.write(str(row['basic_clean_v2']))\n",
    "file = 'path/test/case-9/{}.txt'\n",
    "for i, row in test[test.case == 9].iterrows():\n",
    "    with open(file.format('note_'+str(i)), 'w') as f:\n",
    "        f.write(str(row['basic_clean_v2']))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 37931 files belonging to 10 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-01 16:42:48.824593: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2118 files belonging to 10 classes.\n",
      "Found 2107 files belonging to 10 classes.\n"
     ]
    }
   ],
   "source": [
    "train_dataset = tf.keras.preprocessing.text_dataset_from_directory('path/student_notes/', batch_size=32)\n",
    "validate_dataset = tf.keras.preprocessing.text_dataset_from_directory('path/validate/', batch_size=32)\n",
    "test_dataset = tf.keras.preprocessing.text_dataset_from_directory('path/test/', batch_size=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of batches in train_dataset: 1186\n",
      "Number of batches in validate_dataset: 67\n",
      "Number of batches in test_dataset: 66\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of batches in train_dataset: {train_dataset.cardinality()}')\n",
    "print(f\"Number of batches in validate_dataset: {validate_dataset.cardinality()}\")\n",
    "print(f\"Number of batches in test_dataset: {test_dataset.cardinality()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's preview a few samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'26 yo f comes for a fu on palpitations .  she has been having palpitations x 5 yrs ,  not too often ,  but 3wks ago she was having them 1-2x day ,  lasting 15-30min .  she lost her job 2months ago and bought a condo 3months ago and this has been stressing her .  has decreased consentration but no feeling of sadness or depression .  denies chest pain ,  sob ,  abd pain ,  dizziness ,  h a ,  skin hair changes ,  intolerance to heat ,  sweating  , gu gi s s ,  fever ,  fatigue ,  appetite weight changes  .   pmh ,  all ,  meds ,  psh ,  trauma ,  fh none  gyn lmp 1month ago ,  regular .  4days 30 days .  pap smeal 1yr ago=nl  sh sexually active w  husband only .  no toxic habits .  exercise few x wk .  unmployed ,  looking x a job   '\n",
      "5\n",
      "b'patient is a 35yo male with a past medical history of back pain and muscle spasms with complaints of intermittent epigastric pain which began 2 months ago .  this is the first time he has had this pain and describes it as burning gnawing .  it does not radiate and is a 5 10 in intesity .  tums intially helped the pain but has stopped working .  nothing exacerbates the pain .  patient admits to darker stools which bagan 2 weeks ago .  he has also been burping more than usual lately .  he admits to nausea but denies vomiting or weight changes  pmh - see hpi  meds - motrin for back pain  allergies - none  fh - uncle with ulcers  soc hx - works in construction ,  diet is poor ,  does not exercise ,  smoker for 20 years and is currently using 1 2 pack  day ,  does not consime alcohol  sexual hx - not currently seually active ,  sti screenings negative ,  uses protection  ros - negative except for hpi'\n",
      "3\n",
      "b'mr .  hamilton is a 35 yo african-american male who presents with 2 months of \"stomach issues . \" he has noticed a constant ,  \"gnawing\" pain over is epigastrium without radiation that was initially helped with an antacid (tums) ,  but they no longer appear to help .  he is noting accompanying nausea ,  but has not vomited .  nothing appears to make this pain worse or better .  he has also felt quite \"tired\" over the past few weeks and believes that his stool is softer than normal .  no change in weight ;  although eating less due to bloat .  no mood issues .   ros :  negative except for what is stated above  pmh :  back pain ,  joint pains  past surgical history :  none  medications :  ibuprofen 2 tabs 1x week  allergies :  nkda  social :  high-elevation construction worker ;  no drug use  -smoking :  1 2 - 1 pack per day for the past 20 years ;  not interested in cessation  - etoh :  a few beers per week  diet- mostly fast food  exercise :  3-4 days week  '\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "# It's important to take a look at your raw data to ensure your normalization\n",
    "# and tokenization will work as expected. We can do that by taking a few\n",
    "# examples from the training set and looking at them.\n",
    "# This is one of the places where eager execution shines:\n",
    "# we can just evaluate these tensors using .numpy()\n",
    "# instead of needing to evaluate them in a Session/Graph context.\n",
    "for text_batch, label_batch in train_dataset.take(1):\n",
    "    for i in range(3):\n",
    "        print(text_batch.numpy()[i])\n",
    "        print(label_batch.numpy()[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       ",            429737\n",
       ".            372297\n",
       ":            178467\n",
       "and          126653\n",
       "no           114796\n",
       "              ...  \n",
       "endocrine        21\n",
       "14)              21\n",
       "illicut          21\n",
       "pcos             21\n",
       "pshx-none        21\n",
       "Length: 4722, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = pd.Series(' '.join(train.basic_clean_v2).split()).value_counts()\n",
    "\n",
    "b[b > 20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import TextVectorization\n",
    "import string\n",
    "import re\n",
    "\n",
    "def custom_standardization(input_data):\n",
    "    lowercase = tf.strings.lower(input_data)\n",
    "    stripped_html = tf.strings.regex_replace(lowercase, \"<br />\", \" \")\n",
    "    return tf.strings.regex_replace(\n",
    "        stripped_html, f\"[{re.escape(string.punctuation)}]\", \"\"\n",
    "    )\n",
    "\n",
    "max_features = 5000\n",
    "embedding_dim = 28\n",
    "sequence_length = 170"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorize_layer = TextVectorization(\n",
    "    standardize= custom_standardization,\n",
    "    max_tokens = max_features,\n",
    "    output_mode='int',\n",
    "    output_sequence_length = sequence_length\n",
    ")\n",
    "# Set vocabulary for the text encoder\n",
    "vectorize_layer.adapt(train_dataset.map(lambda text, label: text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's make a text-only dataset (no labels):\n",
    "text_ds = train_dataset.map(lambda x, y: x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorize_layer.adapt(text_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.layers.preprocessing.text_vectorization.TextVectorization at 0x7fbdb4af3d00>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorize_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_text(text, label):\n",
    "    text = tf.expand_dims(text, -1)\n",
    "    return vectorize_layer(text), label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize the data.\n",
    "train_ds = train_dataset.map(vectorize_text)\n",
    "val_ds = validate_dataset.map(vectorize_text)\n",
    "test_ds = test_dataset.map(vectorize_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do async prefetching / buffering of the data for best performance on GPU.\n",
    "train_ds = train_ds.cache().prefetch(buffer_size=10)\n",
    "val_ds = val_ds.cache().prefetch(buffer_size=10)\n",
    "test_ds = test_ds.cache().prefetch(buffer_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 117  232  453   12  198   75  139  514    6   67   27   93   47   43\n",
      "   95    2   10  538  200  476    7   40   20   26   81    4   88  204\n",
      "  400   99  203  156    2  344    5 3417   27   91 1118  556 1283    2\n",
      "   27    5  753  912    5  450  241  437    6  978  680 3621   20   26\n",
      "   19    7   67  491  126  120   73   41   20   26   19    5  670   14\n",
      "    7  445   13    7  593  692  668  123   18 1429    2  616  266  112\n",
      "    5   12  596  386   84  731   18  153    3   66    9    3  110    3\n",
      "  182    9    3  242   29    3   61   29    3   79    3  523  427   74\n",
      "   21  452  636   34   25  379    3  192  142   50   15   23   15   56\n",
      "   15   76  262  301  262  774  260   18  153   20   30   19  330   48\n",
      "    4  195   72   52   75  144  172    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0]\n",
      "5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-01 16:43:03.744544: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    }
   ],
   "source": [
    "for text_batch, label in train_ds.take(1):\n",
    "    for i in range(1):\n",
    "        print(text_batch.numpy()[i])\n",
    "        print(label.numpy()[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "\n",
    "# A integer input for vocab indices.\n",
    "inputs = tf.keras.Input(shape=(None,), dtype=\"int64\")\n",
    "\n",
    "# Next, we add a layer to map those vocab indices into a space of dimensionality\n",
    "# 'embedding_dim'.\n",
    "x = layers.Embedding(max_features, embedding_dim)(inputs)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "\n",
    "# Conv1D + global max pooling\n",
    "x = layers.Conv1D(128, 7, padding=\"valid\", activation=\"relu\", strides=3)(x)\n",
    "x = layers.Conv1D(128, 7, padding=\"valid\", activation=\"relu\", strides=3)(x)\n",
    "x = layers.GlobalMaxPooling1D()(x)\n",
    "\n",
    "# We add a vanilla hidden layer:\n",
    "x = layers.Dense(128, activation=\"relu\")(x)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "\n",
    "# We project onto a single unit output layer, and squash it with a sigmoid:\n",
    "predictions = layers.Dense(1, activation=\"sigmoid\", name=\"predictions\")(x)\n",
    "\n",
    "model = tf.keras.Model(inputs, predictions)\n",
    "\n",
    "# Compile the model with binary crossentropy loss and an adam optimizer.\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "1186/1186 [==============================] - 19s 15ms/step - loss: 0.0000e+00 - accuracy: 0.0191 - val_loss: 0.0000e+00 - val_accuracy: 0.0212\n",
      "Epoch 2/3\n",
      "1186/1186 [==============================] - 14s 12ms/step - loss: 0.0000e+00 - accuracy: 0.0192 - val_loss: 0.0000e+00 - val_accuracy: 0.0212\n",
      "Epoch 3/3\n",
      "1186/1186 [==============================] - 13s 11ms/step - loss: 0.0000e+00 - accuracy: 0.0192 - val_loss: 0.0000e+00 - val_accuracy: 0.0212\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fbdb5963d30>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epochs = 3\n",
    "\n",
    "model.fit(train_ds, validation_data=val_ds, epochs = epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the model on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66/66 [==============================] - 1s 5ms/step - loss: 0.0000e+00 - accuracy: 0.0176\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.0, 0.017560511827468872]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A string input\n",
    "inputs = tf.keras.Input(shape=(1,), dtype=\"string\")\n",
    "# Turn strings into vocab indices\n",
    "indices = vectorize_layer(inputs)\n",
    "# Turn vocab indices into predictions\n",
    "outputs = model(indices)\n",
    "\n",
    "# Our end to end model\n",
    "end_to_end_model = tf.keras.Model(inputs, outputs)\n",
    "end_to_end_model.compile(\n",
    "    loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3067ead486e059ec00ffe7555bdb889e6e264a24dc711bf108106cc7baee8d5d"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
