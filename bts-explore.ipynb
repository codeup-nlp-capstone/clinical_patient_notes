{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "702048fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "from acquire import remove_stopwords, basic_clean, tokenize, prep_and_split_data\n",
    "from prepare_jag import basic_clean3\n",
    "import re\n",
    "from re import search\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow import keras\n",
    "import io\n",
    "import os\n",
    "import shutil\n",
    "import string\n",
    "\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, GlobalAveragePooling1D\n",
    "from tensorflow.keras.layers import TextVectorization\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20f924fb",
   "metadata": {},
   "source": [
    "Download the data from the [Kaggle Competition Site](https://www.kaggle.com/c/medicalnotes-2019/data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fca759d1",
   "metadata": {},
   "source": [
    "# Data Dictionary\n",
    "descriptor: the value held in the 'feature_text' column. These are features that describe the individual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d8f1d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read csv files into a Pandas dataframe.\n",
    "features = pd.read_csv('features.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d4d291f",
   "metadata": {},
   "outputs": [],
   "source": [
    "notes = pd.read_csv('patient_notes.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27479e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6b7bb54",
   "metadata": {},
   "outputs": [],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb0cb442",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get familiar with the 'features' dataframe.\n",
    "features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "959f5220",
   "metadata": {},
   "source": [
    "# Set sights on target:\n",
    "'feature_text' targeted\n",
    "\n",
    "I will have to create a function that will iterate through the students' patient notes and identify the different ways different students express the descriptors.\n",
    "\n",
    "Tentative plan: \n",
    "1. Rename {'case_num':'case', 'feature_text':'target'}\n",
    "2. Rename {'pn_num':'note_id', 'case_num':'case', 'pn_history':'student_notes'}\n",
    "3. Normalize the text in features.feature_text and notes.pn_history.\n",
    "    * clean it\n",
    "4. Create a dataframe that holds the original text and the clean.\n",
    "5. Split data in train, validate, and test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c3f251d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename columns in the features dataframe.\n",
    "features.rename(columns={'feature_num':'feature_id', 'case_num':'case', 'feature_text':'target'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62874eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify\n",
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d7fa10a",
   "metadata": {},
   "outputs": [],
   "source": [
    "features.target.value_counts().head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ff09e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(features.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e45060b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename columns in notes dataframe.\n",
    "notes.rename(columns={'pn_num':'note_id', 'case_num':'case', 'pn_history':'student_notes'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce8b6184",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify\n",
    "notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2515017c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check 'features' dataframe for null values and data types.\n",
    "features.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbb0ed01",
   "metadata": {},
   "source": [
    "# Takeaways\n",
    "* The 'target' column holds values related to the individual patient. \n",
    "* There are no null values and the data types make sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50a7b18c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the type of values in the 'feature_text' column.\n",
    "features.target.value_counts().head(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f91650d",
   "metadata": {},
   "source": [
    "# Takeaways\n",
    "* It seems as if they created a unique list of descriptors for each patient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0471257",
   "metadata": {},
   "outputs": [],
   "source": [
    "features.case.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfcc3e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "features.case.value_counts().describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b955112a",
   "metadata": {},
   "source": [
    "# Takeaways\n",
    "* Descriptors for each patient ranges from 9 - 18.\n",
    "* Average amount of descriptors per patient is 14."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cc9bca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "notes.case.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baee0247",
   "metadata": {},
   "source": [
    "# Takeaways\n",
    "* Student notes for patient_3 has close to 10,000 submissions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6825f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_text2(df, column, extra_words=[], exclude_words=['no','i']):\n",
    "    '''\n",
    "    This function take in a df and the string name for a text column with \n",
    "    option to pass lists for extra_words and exclude_words and\n",
    "    returns a df with the text article title, original text, stemmed text,\n",
    "    lemmatized text, cleaned, tokenized, & lemmatized text with stopwords removed.\n",
    "    '''\n",
    "    df['clean'] = df[column].apply(basic_clean)\\\n",
    "                            .apply(tokenize)\\\n",
    "                            .apply(remove_stopwords, \n",
    "                                   extra_words=extra_words, \n",
    "                                   exclude_words=exclude_words)\n",
    "    return df[['case', column, 'clean']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d381ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "prep_text2(notes, 'student_notes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de7186b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d07b8803",
   "metadata": {},
   "outputs": [],
   "source": [
    "notes.student_notes[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30d83f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "notes.clean[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0d36385",
   "metadata": {},
   "source": [
    "# Takeaways\n",
    "* All of the symbols in the original are causing the cleaned version to produce concatenated words which will must be fixed.\n",
    "* I will use regular expression to convert all symbols into spaces. From there I can locate low value words and add them to the stopword list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fca49b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use regex to substitute everything that is not a number or leter with an empty space.\n",
    "# re.sub(r\"[\\W]\", ' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26438282",
   "metadata": {},
   "source": [
    "# Takeaways\n",
    "* The regex method produces a more coherent output. I will use it on the entire column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d15419e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "notes.student_notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a75d4fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "student_notes_words = ' '.join(notes.clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4c3d739",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' This line of code slows up the notebook. I will keep it commented out for now.'''\n",
    "# Get a peak:\n",
    "# student_notes_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4def864",
   "metadata": {},
   "outputs": [],
   "source": [
    "student_notes_corpus = student_notes_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca4adb9b",
   "metadata": {},
   "source": [
    "### Analyze the student notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5a1b58c",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(student_notes_corpus.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43ead15e",
   "metadata": {},
   "source": [
    "# Takeaways\n",
    "* There is a grand total of 3,990,311 words written by students.\n",
    "* The average reading speed for an adult is 200 - 250 words per minute.\n",
    "* It would take the average person 15961.2 - 19952.60 minutes to read all this.\n",
    "* 266.00 - 332.50 hours.\n",
    "* 11.10 - 13.90 days."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "882642a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_frequencies = pd.Series(student_notes_corpus.split()).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb09414",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_frequencies.tail(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd969d0f",
   "metadata": {},
   "source": [
    "# Takeaway\n",
    "* Most of the words that only show up once are typos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb55d44f",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(pd.Series(student_notes_corpus.split()).unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8117492",
   "metadata": {},
   "source": [
    "# Takeaways\n",
    "* There are 44770 unique words that show up in student notes. Most of these could be typos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aba7042",
   "metadata": {},
   "outputs": [],
   "source": [
    "features.target.head(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a9fe300",
   "metadata": {},
   "source": [
    "## Break down target into individual features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "badd4f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run text through 'basic_clean' function.\n",
    "cleaned_targets = features.target.apply(basic_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c58ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify.\n",
    "cleaned_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60316e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of all individual targets.\n",
    "lists_of_targets = []\n",
    "for target in cleaned_targets:\n",
    "    # This line of code will split targets that have the word 'or' in it at that word.\n",
    "    lists_of_targets.append(list(re.split(r'\\bor', target)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c87cff97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list that separates nested lists. \n",
    "list_of_targets = []\n",
    "for ailments in lists_of_targets:\n",
    "    for ailment in ailments:\n",
    "        list_of_targets.append(ailment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db64270b",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d84d2547",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strip all whitespaces.\n",
    "list_of_targets = [s.strip() for s in list_of_targets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c4927a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fab173f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function completes all the above tasks.\n",
    "def boil_it_down(df, column):\n",
    "    cleaned_column = df[column].apply(basic_clean)\n",
    "    lists_of_targets = []\n",
    "    for target in cleaned_column:\n",
    "        lists_of_targets.append(list(re.split(r'\\bor', target)))\n",
    "    list_of_targets = []\n",
    "    for ailments in lists_of_targets:\n",
    "        for ailment in ailments:\n",
    "            list_of_targets.append(ailment)\n",
    "    list_of_targets = [s.strip() for s in list_of_targets]\n",
    "    return list_of_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19eaad31",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(list_of_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8052f007",
   "metadata": {},
   "outputs": [],
   "source": [
    "boil_it_down(features, 'target')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc26a231",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a for loop that checks for perfect matches.\n",
    "perfect_match = []\n",
    "for ailment in list_of_targets:\n",
    "    for note in notes.clean:\n",
    "        if ailment in note:\n",
    "            perfect_match.append(ailment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6e47871",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(perfect_match).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad809819",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(pd.Series(perfect_match).value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72b043b0",
   "metadata": {},
   "source": [
    "# Takeaways\n",
    "* Out of the 179 unique targets, 97 have shown up, word-for-word, in the student notes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e091bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(re.findall(r\"\\bno\\b\", list_of_targets[6])) + ' ' + list_of_targets[6].split()[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb1c21e",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_targets[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96cab1da",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f5a13c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_list = []\n",
    "for ailment in list_of_targets:\n",
    "    new_list.append(remove_stopwords(ailment, exclude_words = ['no', 'i']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba8ca8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16c1aa0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(new_list[0].split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f8fa53",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_ailment_in_notes = []\n",
    "for ailment in new_list:    \n",
    "    for i in range(len(ailment.split())):\n",
    "        if ailment.split()[i] in notes.clean[0]:\n",
    "            list_of_ailment_in_notes.append(ailment.split()[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21234e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_ailment_in_notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbee7958",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(list_of_ailment_in_notes).unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7cac2ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b68ebdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51cd1cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(cleaned_targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f53356af",
   "metadata": {},
   "source": [
    "## Create a dataframe that combines all targets of a case into one list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1daa501",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the 'basic_clean' function to the targets\n",
    "features['cleaned_targets'] = features.target.apply(basic_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5726399d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of targets for each case\n",
    "case_0_targets = list(features[features.case == 0].cleaned_targets)\n",
    "case_1_targets = list(features[features.case == 1].cleaned_targets)\n",
    "case_2_targets = list(features[features.case == 2].cleaned_targets)\n",
    "case_3_targets = list(features[features.case == 3].cleaned_targets)\n",
    "case_4_targets = list(features[features.case == 4].cleaned_targets)\n",
    "case_5_targets = list(features[features.case == 5].cleaned_targets)\n",
    "case_6_targets = list(features[features.case == 6].cleaned_targets)\n",
    "case_7_targets = list(features[features.case == 7].cleaned_targets)\n",
    "case_8_targets = list(features[features.case == 8].cleaned_targets)\n",
    "case_9_targets = list(features[features.case == 9].cleaned_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "855260f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "case_targets = pd.DataFrame({'case':[n for n in np.arange(10)], 'targets':[case_0_targets,case_1_targets,case_2_targets,case_3_targets,case_4_targets,case_5_targets,case_6_targets,case_7_targets,case_8_targets,case_9_targets]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a520067",
   "metadata": {},
   "outputs": [],
   "source": [
    "case_targets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e47d11d",
   "metadata": {},
   "source": [
    "## Merge the newly created dataframe to the 'notes' dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f6ed0a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = notes.merge(case_targets, how='inner', on='case')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c4d2fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d332542f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.targets[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec357a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.clean[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57e5d0c3",
   "metadata": {},
   "source": [
    "## Use stratified splits to ensure each case exists in the splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07756ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_split = 0.1\n",
    "\n",
    "# Initial train and test split.\n",
    "train_df, test_df = train_test_split(\n",
    "    df, test_size=test_split, stratify=df['case'].values,\n",
    ")\n",
    "\n",
    "# Splitting the test set further into validation and new test set.\n",
    "val_df = test_df.sample(frac=0.5)\n",
    "test_df.drop(val_df.index, inplace=True)\n",
    "\n",
    "print(f\"Number of rows in training set: {len(train_df)}\")\n",
    "print(f\"Number of rows in validation set: {len(val_df)}\")\n",
    "print(f\"Number of rows in test set: {len(test_df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d250fa2",
   "metadata": {},
   "source": [
    "# Prepare and split the full dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8a748634",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows in training set: 37931\n",
      "Number of rows in validation set: 2108\n",
      "Number of rows in test set: 2107\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda3/lib/python3.8/site-packages/pandas/core/frame.py:4308: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  return super().drop(\n"
     ]
    }
   ],
   "source": [
    "train_df, validate_df, test_df = prep_and_split_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bd81737",
   "metadata": {},
   "source": [
    "### Prep and split process:\n",
    "    * Renamed columns.\n",
    "    * Normalize text:\n",
    "        - lowercase\n",
    "        - add space before and after punctuation\n",
    "    * Merged student notes data with feature text\n",
    "    * Split data and stratify on case number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac4d9c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bc6757c",
   "metadata": {},
   "source": [
    "# Takeaways\n",
    "* I just realized I might have a problem because the stopwords are removed from the student notes, but not the targets.\n",
    "* I will run this through for the MVP and do it again with the stopwords removed from the targets as well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1088259",
   "metadata": {},
   "source": [
    "## Multi-label binarization\n",
    "Let's preprocess our labels using the [StringLookup](https://keras.io/api/layers/preprocessing_layers/categorical/string_lookup) layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "594dff6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-28 15:14:06.722979: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "targets = tf.ragged.constant(train_df['targets'].values)\n",
    "lookup = tf.keras.layers.StringLookup(output_mode='multi_hot')\n",
    "lookup.adapt(targets)\n",
    "vocab = lookup.get_vocabulary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6dcf2bf4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37931"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_df.targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4975bc4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[UNK]',\n",
       " 'female',\n",
       " 'nausea',\n",
       " '35 year',\n",
       " 'male',\n",
       " 'post prandial bloating or fullness with meals',\n",
       " 'nsaid use or nonsteroidal anti inflammatory drug use',\n",
       " 'no blood in stool',\n",
       " 'minimal to no change with tums',\n",
       " 'intermittent',\n",
       " 'getting worse or progressive or symptoms now daily',\n",
       " 'fhx of pud or family history of peptic ulcer disease',\n",
       " 'epigastric discomfort',\n",
       " 'duration 2 months',\n",
       " 'darker bowel movements',\n",
       " 'burning or gnawing or burning and gnawing',\n",
       " 'awakens at night',\n",
       " '2 to 3 beers a week',\n",
       " 'recent visit to emergency department with negative workup',\n",
       " 'onset 5 years ago',\n",
       " 'no illicit drug use',\n",
       " 'no chest pain',\n",
       " 'no caffeine use',\n",
       " 'increased stress',\n",
       " 'increased frequency recently',\n",
       " 'feels hot or feels clammy',\n",
       " 'fatigue or difficulty concentrating',\n",
       " 'episodes of heart racing',\n",
       " 'episodes last 15 to 30 minutes',\n",
       " 'episode of hand numbness or episode of finger numbness',\n",
       " 'associated throat tightness',\n",
       " 'associated sob or associated shortness of breath',\n",
       " 'associated nausea',\n",
       " 'associated feeling of impending doom',\n",
       " '26 year',\n",
       " '20 year',\n",
       " 'weight stable',\n",
       " 'stress due to caring for elderly parents',\n",
       " 'no depressed mood',\n",
       " 'lack of other thyroid symptoms',\n",
       " 'insomnia',\n",
       " 'heavy caffeine use',\n",
       " 'decreased appetite',\n",
       " 'anxious or nervous',\n",
       " '45 year',\n",
       " 'vomiting',\n",
       " 'viral symptoms or rhinorrhea or scratchy throat',\n",
       " 'subjective fever',\n",
       " 'shares an apartment',\n",
       " 'photophobia',\n",
       " 'no relief with motrin or no relief with tylenol',\n",
       " 'no rash',\n",
       " 'no known illness contacts',\n",
       " 'neck pain',\n",
       " 'myalgias',\n",
       " 'meningococcal vaccine status unknown',\n",
       " 'global headache or diffuse headache',\n",
       " 'family history of migraines',\n",
       " '1 day duration or 2 days duration',\n",
       " 'visual hallucination once',\n",
       " 'unsuccessful napping',\n",
       " 'tossing and turning',\n",
       " 'son died 3 weeks ago',\n",
       " 'sleeping medication ineffective',\n",
       " 'no suicidal ideations',\n",
       " 'loss of interest',\n",
       " 'increased appetite',\n",
       " 'hallucinations after taking ambien',\n",
       " 'fhx of depression or family history of depression',\n",
       " 'early wakening',\n",
       " 'duration 3 weeks',\n",
       " 'diminished energy or feeling drained',\n",
       " 'difficulty with sleep',\n",
       " 'difficulty falling asleep',\n",
       " 'auditory hallucination once',\n",
       " '67 year',\n",
       " 'weight gain',\n",
       " 'unprotected sex',\n",
       " 'symptoms for 6 months',\n",
       " 'last menstrual period 2 months ago',\n",
       " 'infertility hx or infertility history',\n",
       " 'heavy periods or irregular periods',\n",
       " 'fatigue',\n",
       " '17 year',\n",
       " 'shortness of breath',\n",
       " 'no hair changes or no nail changes or no temperature intolerance',\n",
       " 'lightheaded',\n",
       " 'intermittent symptoms',\n",
       " 'heart pounding or heart racing',\n",
       " 'few months duration',\n",
       " 'family history of thyroid disorder',\n",
       " 'family history of mi or family history of myocardial infarction',\n",
       " 'chest pressure',\n",
       " 'caffeine use',\n",
       " 'adderall use',\n",
       " 'vaginal dryness',\n",
       " 'stress',\n",
       " 'sleep disturbance or early awakenings',\n",
       " 'sexually active',\n",
       " 'recent nausea vomiting or recent flulike symptoms',\n",
       " 'prior normal periods',\n",
       " 'onset 3 years ago',\n",
       " 'no premenstrual symptoms',\n",
       " 'lmp 2 months ago or last menstrual period 2 months ago',\n",
       " 'last pap smear i year ago',\n",
       " 'iud',\n",
       " 'irregular menses',\n",
       " 'irregular flow or irregular frequency or irregular intervals',\n",
       " 'hot flashes',\n",
       " 'heavy sweating',\n",
       " '44 year',\n",
       " 'worse with deep breath or pleuritic',\n",
       " 'subjective fevers',\n",
       " 'sharp or stabbing or 7 to 8 out of 10 on pain scale',\n",
       " 'recent upper respiratory symptoms',\n",
       " 'recent heavy lifting at work or recent rock climbing',\n",
       " 'no shortness of breath',\n",
       " 'no relief with asthma inhaler',\n",
       " 'exercise induced asthma',\n",
       " 'duration x 1 day',\n",
       " 'chest pain',\n",
       " 'weight loss',\n",
       " 'right sided lq abdominal pain or right lower quadrant abdominal pain',\n",
       " 'recurrent bouts over past 6 months',\n",
       " 'prior episodes of diarrhea',\n",
       " 'not sexually active',\n",
       " 'normal lmp 2 weeks ago or normal last menstrual period 2 weeks ago',\n",
       " 'no vaginal discharge',\n",
       " 'no urinary symptoms',\n",
       " 'no bloody bowel movements',\n",
       " 'diminished appetite',\n",
       " '8 to 10 hours of acute pain']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2080776b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary:\n",
      "\n",
      "['[UNK]', 'female', 'nausea', '35 year', 'male', 'post prandial bloating or fullness with meals', 'nsaid use or nonsteroidal anti inflammatory drug use', 'no blood in stool', 'minimal to no change with tums', 'intermittent', 'getting worse or progressive or symptoms now daily', 'fhx of pud or family history of peptic ulcer disease', 'epigastric discomfort', 'duration 2 months', 'darker bowel movements', 'burning or gnawing or burning and gnawing', 'awakens at night', '2 to 3 beers a week', 'recent visit to emergency department with negative workup', 'onset 5 years ago', 'no illicit drug use', 'no chest pain', 'no caffeine use', 'increased stress', 'increased frequency recently', 'feels hot or feels clammy', 'fatigue or difficulty concentrating', 'episodes of heart racing', 'episodes last 15 to 30 minutes', 'episode of hand numbness or episode of finger numbness', 'associated throat tightness', 'associated sob or associated shortness of breath', 'associated nausea', 'associated feeling of impending doom', '26 year', '20 year', 'weight stable', 'stress due to caring for elderly parents', 'no depressed mood', 'lack of other thyroid symptoms', 'insomnia', 'heavy caffeine use', 'decreased appetite', 'anxious or nervous', '45 year', 'vomiting', 'viral symptoms or rhinorrhea or scratchy throat', 'subjective fever', 'shares an apartment', 'photophobia', 'no relief with motrin or no relief with tylenol', 'no rash', 'no known illness contacts', 'neck pain', 'myalgias', 'meningococcal vaccine status unknown', 'global headache or diffuse headache', 'family history of migraines', '1 day duration or 2 days duration', 'visual hallucination once', 'unsuccessful napping', 'tossing and turning', 'son died 3 weeks ago', 'sleeping medication ineffective', 'no suicidal ideations', 'loss of interest', 'increased appetite', 'hallucinations after taking ambien', 'fhx of depression or family history of depression', 'early wakening', 'duration 3 weeks', 'diminished energy or feeling drained', 'difficulty with sleep', 'difficulty falling asleep', 'auditory hallucination once', '67 year', 'weight gain', 'unprotected sex', 'symptoms for 6 months', 'last menstrual period 2 months ago', 'infertility hx or infertility history', 'heavy periods or irregular periods', 'fatigue', '17 year', 'shortness of breath', 'no hair changes or no nail changes or no temperature intolerance', 'lightheaded', 'intermittent symptoms', 'heart pounding or heart racing', 'few months duration', 'family history of thyroid disorder', 'family history of mi or family history of myocardial infarction', 'chest pressure', 'caffeine use', 'adderall use', 'vaginal dryness', 'stress', 'sleep disturbance or early awakenings', 'sexually active', 'recent nausea vomiting or recent flulike symptoms', 'prior normal periods', 'onset 3 years ago', 'no premenstrual symptoms', 'lmp 2 months ago or last menstrual period 2 months ago', 'last pap smear i year ago', 'iud', 'irregular menses', 'irregular flow or irregular frequency or irregular intervals', 'hot flashes', 'heavy sweating', '44 year', 'worse with deep breath or pleuritic', 'subjective fevers', 'sharp or stabbing or 7 to 8 out of 10 on pain scale', 'recent upper respiratory symptoms', 'recent heavy lifting at work or recent rock climbing', 'no shortness of breath', 'no relief with asthma inhaler', 'exercise induced asthma', 'duration x 1 day', 'chest pain', 'weight loss', 'right sided lq abdominal pain or right lower quadrant abdominal pain', 'recurrent bouts over past 6 months', 'prior episodes of diarrhea', 'not sexually active', 'normal lmp 2 weeks ago or normal last menstrual period 2 weeks ago', 'no vaginal discharge', 'no urinary symptoms', 'no bloody bowel movements', 'diminished appetite', '8 to 10 hours of acute pain']\n"
     ]
    }
   ],
   "source": [
    "def invert_multi_hot(encoded_labels):\n",
    "    '''Reverse a single multi-hot encoded label to a tuple of vocab terms.'''\n",
    "    hot_indices = np.argwhere(encoded_labels == 1.0)[...,0]\n",
    "    return np.take(vocab, hot_indices)\n",
    "\n",
    "print('Vocabulary:\\n')\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5a06102",
   "metadata": {},
   "source": [
    "### Separate the individual targets from the label pool and then use it to represent a given label set with 0's and 1's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3fc3ebd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original label: ['prior normal periods', 'last pap smear i year ago', 'iud', 'sexually active', 'vaginal dryness', 'irregular menses', 'recent nausea vomiting or recent flulike symptoms', 'no premenstrual symptoms', 'female', 'stress', 'lmp 2 months ago or last menstrual period 2 months ago', 'hot flashes', 'irregular flow or irregular frequency or irregular intervals', 'onset 3 years ago', 'heavy sweating', 'sleep disturbance or early awakenings', '44 year']\n",
      "Label-binarized representation: [[0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.\n",
      "  1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "sample_label = train_df['targets'].iloc[0]\n",
    "print(f'Original label: {sample_label}')\n",
    "\n",
    "label_binarized = lookup([sample_label])\n",
    "print(f'Label-binarized representation: {label_binarized}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0028c57b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 132), dtype=float32, numpy=\n",
       "array([[0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0.]], dtype=float32)>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_binarized"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf341426",
   "metadata": {},
   "source": [
    "## Data preprocessing and [tf.data.Dataset](https://www.tensorflow.org/api_docs/python/tf/data/Dataset) objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9c0cfe46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    37931.000000\n",
       "mean        97.789486\n",
       "std         16.338358\n",
       "min          7.000000\n",
       "25%         88.000000\n",
       "50%        100.000000\n",
       "75%        109.000000\n",
       "max        150.000000\n",
       "Name: clean, dtype: float64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['clean'].apply(lambda x: len(x.split(\" \"))).describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75a69984",
   "metadata": {},
   "source": [
    "# Takeaways\n",
    "* Half of the student notes have a length of 97 words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2a0dcecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seqlen = 97\n",
    "batch_size = 128\n",
    "padding_token = '<pad>'\n",
    "auto = tf.data.AUTOTUNE\n",
    "\n",
    "def make_dataset(dataframe, is_train=True):\n",
    "    labels = tf.ragged.constant(dataframe['targets'].values)\n",
    "    label_binarized = lookup(labels).numpy()\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(\n",
    "        (dataframe['clean'].values, label_binarized)\n",
    "    )\n",
    "    dataset = dataset.shuffle(batch_size * 10) if is_train else dataset\n",
    "    return dataset.batch(batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04939505",
   "metadata": {},
   "source": [
    "## Prepare the [tf.data.Dataset](https://www.tensorflow.org/api_docs/python/tf/data/Dataset) objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6a187452",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = make_dataset(train_df, is_train=True)\n",
    "validate_dataset = make_dataset(validate_df, is_train=False)\n",
    "test_dataset = make_dataset(test_df, is_train=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4513df40",
   "metadata": {},
   "source": [
    "## Preview the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ad600777",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Student note: b'mr hamilton 35 yo presents 2 month history gnawing burning epigastric pain rates pain 5 10 frequency pain increasing 2 times day waking 3 times per week night pain associated nausea bloating emesis noticed melena last 2 weeks also decreased appetite due pain noticed weight loss fever sob fatigue tums used help pain stopped working takes motrin 1 time per week currently several stressors including divorce working high heights construction job concerned pain begun awakening sleep pt counseled ros negative except meds pmh psh fh sh no medical problems no surgeries uncle bleeding ulcer current 5 1 ppd smoker x20 yrs'\n",
      "Targets: ['nausea' '35 year' 'male' 'post prandial bloating or fullness with meals'\n",
      " 'nsaid use or nonsteroidal anti inflammatory drug use'\n",
      " 'no blood in stool' 'minimal to no change with tums' 'intermittent'\n",
      " 'getting worse or progressive or symptoms now daily'\n",
      " 'fhx of pud or family history of peptic ulcer disease'\n",
      " 'epigastric discomfort' 'duration 2 months' 'darker bowel movements'\n",
      " 'burning or gnawing or burning and gnawing' 'awakens at night'\n",
      " '2 to 3 beers a week']\n",
      " \n",
      "Student note: b'35 yo h epigastric pain past 2 months pain progressively getting worse would occur 1x week 2 3x day lasting 1 2 hours episode 5 10 burning gnawing pain non radiating alleviated beginning tums no longer help no aggravating factors associated bloated feeling noticed bowel movements become darker fatigue nausea decreased appetite increased burping associated foods eating patterns review systems neg recent travel trauma fever chills night sweats urinary changes edema weight changes skin changes pmh back muscle strain no surgeries allergies nkda meds motrin prn muscle pain fh bleeding ulcer uncle sh beers week works construction 0 5 1 pack cigarettes day since age 15 precontemplation stage quitting no drugs'\n",
      "Targets: ['nausea' '35 year' 'male' 'post prandial bloating or fullness with meals'\n",
      " 'nsaid use or nonsteroidal anti inflammatory drug use'\n",
      " 'no blood in stool' 'minimal to no change with tums' 'intermittent'\n",
      " 'getting worse or progressive or symptoms now daily'\n",
      " 'fhx of pud or family history of peptic ulcer disease'\n",
      " 'epigastric discomfort' 'duration 2 months' 'darker bowel movements'\n",
      " 'burning or gnawing or burning and gnawing' 'awakens at night'\n",
      " '2 to 3 beers a week']\n",
      " \n",
      "Student note: b'hpi 26 female presenting palpitations palpitations first noticed 5 years ago intermittent 3 weeks ago episodes became frequent presented ed found workup ekg cbc cmp cardiac enzymes wnl patient states episodes occur several times day no pattern feels sob diaphoretic time reports feels like may pass loc no head trauma one occasion numbess fingers hands subsided palpitations patient reports increased stress lost job 1 month ago ros negative except pmh none psh none meds none allergy nkda family noncontributory social lives alone currently relationship boyfriend sexually active no concerns domestic issues non smoker non drinker no illicit drug use'\n",
      "Targets: ['female' 'recent visit to emergency department with negative workup'\n",
      " 'onset 5 years ago' 'no illicit drug use' 'no chest pain'\n",
      " 'no caffeine use' 'increased stress' 'increased frequency recently'\n",
      " 'feels hot or feels clammy' 'fatigue or difficulty concentrating'\n",
      " 'episodes of heart racing' 'episodes last 15 to 30 minutes'\n",
      " 'episode of hand numbness or episode of finger numbness'\n",
      " 'associated throat tightness'\n",
      " 'associated sob or associated shortness of breath' 'associated nausea'\n",
      " 'associated feeling of impending doom' '26 year']\n",
      " \n",
      "Student note: b'ms wicks 67 year old female presents difficulty sleeping first noticed difficulty three weeks ago shortly unexpected loss son consistent since endorses difficulty falling staying asleep ambien help endorses decreased lack interest things typically also endorses decreased energy increased appetite happened ros ms wicks hearing neighbor parties night reportedly occurring reports seeing deceased son sitting kitchen table pmhx htn breast cancer remission 10 past surgical hx lumpectomy 10y ago laparotomy appendicitis fhx father passed stroke 75 htn hypercholesterolemia mom 90 history depression sexual active husband social 2 4 glasses alcohol 2 3x week med hctz lisinopril allergies none'\n",
      "Targets: ['female' 'visual hallucination once' 'unsuccessful napping'\n",
      " 'tossing and turning' 'son died 3 weeks ago'\n",
      " 'sleeping medication ineffective' 'no suicidal ideations'\n",
      " 'loss of interest' 'increased appetite'\n",
      " 'hallucinations after taking ambien'\n",
      " 'fhx of depression or family history of depression' 'early wakening'\n",
      " 'duration 3 weeks' 'diminished energy or feeling drained'\n",
      " 'difficulty with sleep' 'difficulty falling asleep'\n",
      " 'auditory hallucination once' '67 year']\n",
      " \n",
      "Student note: b'hpi past five years patient acute attacks palpitations last several minutes dissipate previously palpitations occurred one two times per month palpitations occur every days patient also notices nausea numbness fingers attacks pt reports worried past several months due buying condo setting attacks occurred frequently patient presented ed attack two weeks ago cbc electrolytes ecg within normal limits ros denies headaches fever chills diarrhea vomiting change strength change appetite change energy pmh no prior medical problems psh none meds none allergies none fh no reported family history cardiac problems medical conditions sh denies alcohol drugs tobacco sexually active boyfriend uses condoms contraception'\n",
      "Targets: ['female' 'recent visit to emergency department with negative workup'\n",
      " 'onset 5 years ago' 'no illicit drug use' 'no chest pain'\n",
      " 'no caffeine use' 'increased stress' 'increased frequency recently'\n",
      " 'feels hot or feels clammy' 'fatigue or difficulty concentrating'\n",
      " 'episodes of heart racing' 'episodes last 15 to 30 minutes'\n",
      " 'episode of hand numbness or episode of finger numbness'\n",
      " 'associated throat tightness'\n",
      " 'associated sob or associated shortness of breath' 'associated nausea'\n",
      " 'associated feeling of impending doom' '26 year']\n",
      " \n"
     ]
    }
   ],
   "source": [
    "text_batch, label_batch = next(iter(train_dataset))\n",
    "\n",
    "for i, text in enumerate(text_batch[:5]):\n",
    "    label = label_batch[i].numpy()[None, ...]\n",
    "    print(f'Student note: {text}')\n",
    "    print(f'Targets: {invert_multi_hot(label[0])}')\n",
    "    print(' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6a9f0fc",
   "metadata": {},
   "source": [
    "## Vectorization\n",
    "Vectorize the text to represent it as a quantitative value. We will use [TextVectorization layer](https://keras.io/api/layers/preprocessing_layers/text/text_vectorization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f8f88864",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42172\n"
     ]
    }
   ],
   "source": [
    "# Get unique words in student notes.\n",
    "vocabulary = set()\n",
    "train_df['clean'].str.split().apply(vocabulary.update)\n",
    "vocabulary_size = len(vocabulary)\n",
    "print(vocabulary_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90613827",
   "metadata": {},
   "source": [
    "## Now we create our vectorization layer and map() to the [tf.data.Datasets](https://www.tensorflow.org/api_docs/python/tf/data/Dataset) created earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6498e013",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"/usr/local/anaconda3/lib/python3.8/site-packages/keras/engine/base_preprocessing_layer.py\", line 118, in adapt_step  *\n        self.update_state(data)\n    File \"/usr/local/anaconda3/lib/python3.8/site-packages/keras/layers/preprocessing/text_vectorization.py\", line 431, in update_state  **\n        self._lookup_layer.update_state(self._preprocess(data))\n    File \"/usr/local/anaconda3/lib/python3.8/site-packages/keras/layers/preprocessing/text_vectorization.py\", line 520, in _preprocess\n        raise ValueError(\n\n    ValueError: When using `TextVectorization` to tokenize strings, the input rank must be 1 or the last shape dimension must be 1. Received: inputs.shape=(None, 42172) with rank=2\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/jl/s3ptdwdx55v01d2g2wrs7vdc0000gn/T/ipykernel_15071/4089501721.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/CPU:0'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mtext_vectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madapt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m train_dataset = train_dataset.map(\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.8/site-packages/keras/layers/preprocessing/text_vectorization.py\u001b[0m in \u001b[0;36madapt\u001b[0;34m(self, data, batch_size, steps)\u001b[0m\n\u001b[1;32m    426\u001b[0m           \u001b[0margument\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msupported\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0marray\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    427\u001b[0m     \"\"\"\n\u001b[0;32m--> 428\u001b[0;31m     \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madapt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    429\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    430\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mupdate_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.8/site-packages/keras/engine/base_preprocessing_layer.py\u001b[0m in \u001b[0;36madapt\u001b[0;34m(self, data, batch_size, steps)\u001b[0m\n\u001b[1;32m    247\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcatch_stop_iteration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    248\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 249\u001b[0;31m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_adapt_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    250\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m             \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.8/site-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mautograph_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1145\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1146\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ag_error_metadata\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1147\u001b[0;31m               \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1148\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1149\u001b[0m               \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/usr/local/anaconda3/lib/python3.8/site-packages/keras/engine/base_preprocessing_layer.py\", line 118, in adapt_step  *\n        self.update_state(data)\n    File \"/usr/local/anaconda3/lib/python3.8/site-packages/keras/layers/preprocessing/text_vectorization.py\", line 431, in update_state  **\n        self._lookup_layer.update_state(self._preprocess(data))\n    File \"/usr/local/anaconda3/lib/python3.8/site-packages/keras/layers/preprocessing/text_vectorization.py\", line 520, in _preprocess\n        raise ValueError(\n\n    ValueError: When using `TextVectorization` to tokenize strings, the input rank must be 1 or the last shape dimension must be 1. Received: inputs.shape=(None, 42172) with rank=2\n"
     ]
    }
   ],
   "source": [
    "text_vectorizer = layers.TextVectorization(\n",
    "    max_tokens=vocabulary_size, ngrams=2, output_mode='tf_idf'\n",
    ")\n",
    "\n",
    "with tf.device('/CPU:0'):\n",
    "    text_vectorizer.adapt(train_dataset.map(lambda text, label: text))\n",
    "\n",
    "train_dataset = train_dataset.map(\n",
    "    lambda text, label: (text_vectorizer(text), label), num_parallel_calls=auto).prefetch(auto)\n",
    "validate_dataset = validate_dataset.map(\n",
    "    lambda text, label: (text_vectorizer(text), label), num_parallel_calls=auto).prefetch(auto)\n",
    "test_dataset = test_dataset.map(\n",
    "    lambda text, label: (text_vectorizer(text), label), num_parallel_calls=auto).prefetch(auto)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f68cd293",
   "metadata": {},
   "source": [
    "## Create a text classification model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3824f2d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model():\n",
    "    shallow_mlp_model = keras.Sequential(\n",
    "        [\n",
    "            layers.Dense(512, activation=\"relu\"),\n",
    "            layers.Dense(256, activation=\"relu\"),\n",
    "            layers.Dense(lookup.vocabulary_size(), activation=\"softmax\"),\n",
    "        ]  # More on why \"sigmoid\" has been used here in a moment.\n",
    "    )\n",
    "    return shallow_mlp_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec67c1ef",
   "metadata": {},
   "source": [
    "## Train our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c688f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "\n",
    "shallow_mlp_model = make_model()\n",
    "shallow_mlp_model.compile(\n",
    "    loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"categorical_accuracy\"]\n",
    ")\n",
    "\n",
    "history = shallow_mlp_model.fit(\n",
    "    train_dataset, validation_data=validation_dataset, epochs=epochs\n",
    ")\n",
    "\n",
    "\n",
    "def plot_result(item):\n",
    "    plt.plot(history.history[item], label=item)\n",
    "    plt.plot(history.history[\"val_\" + item], label=\"val_\" + item)\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(item)\n",
    "    plt.title(\"Train and Validation {} Over Epochs\".format(item), fontsize=14)\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_result(\"loss\")\n",
    "plot_result(\"categorical_accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b30d508",
   "metadata": {},
   "source": [
    "## Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fa709bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, categorical_acc = shallow_mlp_model.evaluate(test_dataset)\n",
    "print(f\"Categorical accuracy on the test set: {round(categorical_acc * 100, 2)}%.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ce8dca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a model for inference.\n",
    "model_for_inference = keras.Sequential([text_vectorizer, shallow_mlp_model])\n",
    "\n",
    "# Create a small dataset just for demoing inference.\n",
    "inference_dataset = make_dataset(test_df.sample(100), is_train=False)\n",
    "text_batch, label_batch = next(iter(inference_dataset))\n",
    "predicted_probabilities = model_for_inference.predict(text_batch)\n",
    "\n",
    "# Perform inference.\n",
    "for i, text in enumerate(text_batch[0]):\n",
    "    label = label_batch[i].numpy()[None, ...]\n",
    "    print(f\"Student notes: {text}\")\n",
    "    print(f\"Targets(s): {invert_multi_hot(label[0])}\")\n",
    "    predicted_proba = [proba for proba in predicted_probabilities[i]]\n",
    "    top_15_labels = [\n",
    "        x\n",
    "        for _, x in sorted(\n",
    "            zip(predicted_probabilities[i], lookup.get_vocabulary()),\n",
    "            key=lambda pair: pair[0],\n",
    "            reverse=True,\n",
    "            \n",
    "        )\n",
    "    ][:15]\n",
    "    print(f\"Predicted Targets(s): ({', '.join([label for label in all_labels])})\")\n",
    "    print(\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dc0b091",
   "metadata": {},
   "source": [
    "# Takeaways\n",
    "* My model seems to be predicting every target for every case. I will have to look into this tomorrow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65ff2884",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6a0a6c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "336115eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71e07812",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b22d75f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f63d0803",
   "metadata": {},
   "source": [
    "# Here I will conduct word embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dc7d48e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e46ef0d0",
   "metadata": {},
   "source": [
    "## Let's try on case 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84ff0735",
   "metadata": {},
   "outputs": [],
   "source": [
    "case0 = train_df[train_df.case == 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4fb3631",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
